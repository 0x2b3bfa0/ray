{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5705f2",
   "metadata": {},
   "source": [
    "(streamlit-serve-tutorial)=\n",
    "\n",
    "# Building a Streamlit app with Ray Serve\n",
    "\n",
    "In this example, we will show you how to wrap a machine learning model served\n",
    "by Ray Serve in a [Streamlit application](https://streamlit.io/).\n",
    "\n",
    "Specifically, we're going to download a GPT-2 model from the `transformer` library,\n",
    "define a Ray Serve deployment with it, and then define and launch a Streamlit app.\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017f8c4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install all dependencies for this example.\n",
    "! pip install ray streamlit transformers requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245b4c3",
   "metadata": {},
   "source": [
    "## Deploying a model with Ray Serve\n",
    "\n",
    "To start off, we import Ray Serve and Streamlit, as well as the `transformers` and `requests` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d354ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from ray import serve\n",
    "from transformers import pipeline\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1eba9",
   "metadata": {},
   "source": [
    "Next, we define a Ray Serve deployment with a GPT-2 model, by using the `@serve.deployment` decorator on a `model`\n",
    "function that takes a `request` argument.\n",
    "In this function we define a GPT-2 model with a call to `pipeline` and return the result of querying the model.\n",
    "Before defining the deployment, we start Ray Serve using `serve.start()`, and then proceed to deploy the model\n",
    "with `model.deploy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' not in st.session_state:\n",
    "    serve.start()\n",
    "\n",
    "    @serve.deployment\n",
    "    def model(request):\n",
    "        language_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "        query = request.query_params[\"query\"]\n",
    "        return language_model(query, max_length=100)\n",
    "\n",
    "    model.deploy()\n",
    "    st.session_state['model'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7be609",
   "metadata": {},
   "source": [
    "Note that we're using Streamlit's `session_state` to make sure the deployment only gets run once.\n",
    "If we didn't use such a mechanism, Streamlit would simply run the whole script again, which is not what we want.\n",
    "\n",
    "To test this deployment we use a simple `example` query to get a `response` from the model running\n",
    "on `localhost:8000/model`.\n",
    "The first time you use this endpoint, the model will be downloaded first, which can take a while to complete.\n",
    "Subsequent calls will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"What's the meaning of life?\"\n",
    "response = requests.get(f\"http://localhost:8000/model?query={example}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11e675",
   "metadata": {},
   "source": [
    "## Defining and launching a Streamlit app\n",
    "\n",
    "To define a streamlit app, let's first create a convenient wrapper that takes a `query` argument and returns\n",
    "the result of querying the GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(query):\n",
    "    response = requests.get(f\"http://localhost:8000/model?query={query}\")\n",
    "    return response.json()[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4a5ef",
   "metadata": {},
   "source": [
    "Apart from this `gpt2` function, the only other thing that we need is a way for users to specify the model input,\n",
    "and a way to display the result.\n",
    "Since our model takes text as input and output, this turns out to be pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115fb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Serving a GPT-2 model\")\n",
    "\n",
    "query = st.text_input(label=\"Input prompt\", value=\"What's the meaning of life?\")\n",
    "\n",
    "if st.button('Run model'):\n",
    "    output = gpt2(query)\n",
    "\n",
    "    st.header(\"Model output\")\n",
    "    st.text(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e998109",
   "metadata": {},
   "source": [
    "To serve this model with Streamlit, we use just a few simple text components, namely `st.title`, `st.header`, and\n",
    "`st.text` for output and `st.text_input` for getting the model input.\n",
    "We also use a button to trigger model inference for a new input prompt.\n",
    "There's much more you can do with Streamlit, but this is just a simple example.\n",
    "\n",
    "```{margin}\n",
    "The [Streamlit API documentation](https://docs.streamlit.io/library/api-reference)\n",
    "covers all viable Streamlit components in detail.\n",
    "```\n",
    "\n",
    "Finally, if you put everything we just did together in a single file called `streamlit_app.py`,\n",
    "you can run your Streamlit app with Ray Serve as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ce70e",
   "metadata": {
    "pycharm": {
     "name": "#%% bash\n"
    }
   },
   "outputs": [],
   "source": [
    "streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5638a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This should launch an interface that you can interact with that looks like this:\n",
    "\n",
    "```{image} https://raw.githubusercontent.com/ray-project/images/master/docs/serve/streamlit_serve_gpt.png\n",
    "```\n",
    "\n",
    "To summarize, if you know the basics of Streamlit, it's straightforward to deploy a model with Ray Serve with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}