from ray.rllib.algorithms.ddppo import DDPPO as DDPPOTrainer  # noqa
from ray.rllib.algorithms.ddppo import DEFAULT_CONFIG
