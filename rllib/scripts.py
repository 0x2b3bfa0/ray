#!/usr/bin/env python
import argparse
from enum import Enum
import typer

from ray.tune.result import DEFAULT_RESULTS_DIR
from ray.tune.experiment.config_parser import _make_parser


class Framework(str, Enum):
    tf = "tf"
    tf2 = "tf2"
    tfe = "tfe"
    torch = "torch"


def _create_tune_parser_help():
    """Create a Tune dummy parser to access its 'help' docstrings."""
    parser = _make_parser(
        parser_creator=None,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    return parser.__dict__.get("_option_string_actions")


PARSER_HELP = _create_tune_parser_help()


def get_help(key: str) -> str:
    """Get the help string from a parser for a given key.
    If e.g. 'resource_group' is provided, we return the entry for '--resource-group'."""
    key = "--" + key
    key = key.replace("_", "-")
    if not key in PARSER_HELP.keys():
        raise ValueError(f"Key {key} not found in parser.")
    return PARSER_HELP.get(key).help


eval_help = dict(
    checkpoint="Optional checkpoint from which to roll out. If none provided, we will "
    "evaluate an untrained algorithm.",
    run="The algorithm or model to train. This may refer to the name of a built-in "
    "Algorithm (e.g. RLlib's `DQN` or `PPO`), or a user-defined trainable "
    "function or class registered in the Tune registry.",
    env="The environment specifier to use. This could be an openAI gym "
    "specifier (e.g. `CartPole-v1`) or a full class-path (e.g. "
    "`ray.rllib.examples.env.simple_corridor.SimpleCorridor`).",
    local_mode="Run Ray in local mode for easier debugging.",
    render="Render the environment while evaluating. Off by default",
    video_dir="Specifies the directory into which videos of all episode"
    "rollouts will be stored.",
    steps="Number of time-steps to roll out. The evaluation will also stop if "
    "`--episodes` limit is reached first. A value of 0 means no "
    "limitation on the number of time-steps run.",
    episodes="Number of complete episodes to roll out. The evaluation will also stop "
    "if `--steps` (time-steps) limit is reached first. A value of 0 means "
    "no limitation on the number of episodes run.",
    out="Output filename",
    config="Algorithm-specific configuration (e.g. `env`, `framework` etc.). "
    "Gets merged with loaded configuration from checkpoint file and "
    "`evaluation_config` settings therein.",
    save_info="Save the info field generated by the step() method, "
    "as well as the action, observations, rewards and done fields.",
    use_shelve="Save rollouts into a Python shelf file (will save each episode "
    "as it is generated). An output filename must be set using --out.",
    track_progress="Write progress to a temporary file (updated "
    "after each episode). An output filename must be set using --out; "
    "the progress file will live in the same folder.",
)

train_help = dict(
    env="The environment specifier to use. This could be an openAI gym "
    "specifier (e.g. `CartPole-v1`) or a full class-path (e.g. "
    "`ray.rllib.examples.env.simple_corridor.SimpleCorridor`).",
    config_file="If specified, use config options from this file. Note that this "
    "overrides any trial-specific options set via other flags.",
    experiment_name="Name of the subdirectory under `local_dir` to put results in.",
    framework="The identifier of the deep learning framework you want to use."
    "Choose between TensorFlow 1.x ('tf'), TensorFlow 2.x ('tf2'), "
    "TensorFlow 1.x in eager mode ('tfe'), and PyTorch ('torch').",
    v="Whether to use INFO level logging.",
    vv="Whether to use DEBUG level logging.",
    resume="Whether to attempt to resume from previous experiments.",
    local_dir=f"Local dir to save training results to. "
    f"Defaults to '{DEFAULT_RESULTS_DIR}'.",
    local_mode="Run Ray in local mode for easier debugging.",
    ray_address="Connect to an existing Ray cluster at this address instead "
    "of starting a new one.",
    ray_ui="Whether to enable the Ray web UI.",
    ray_num_cpus="The '--num-cpus' argument to use if starting a new cluster.",
    ray_num_gpus="The '--num-gpus' argument to use if starting a new cluster.",
    ray_num_nodes="Emulate multiple cluster nodes for debugging.",
    ray_object_store_memory="--object-store-memory to use if starting a new cluster.",
    upload_dir="Optional URI to sync training results to (e.g. s3://bucket).",
    trace="Whether to attempt to enable tracing for eager mode.",
    torch="Whether to use PyTorch (instead of tf) as the DL framework. "
    "This argument is deprecated, please use --framework to select 'torch'"
    "as backend.",
    eager="Whether to attempt to enable TensorFlow eager execution. "
    "This argument is deprecated, please choose between 'tfe' and 'tf2' in "
    "--framework to run select eager mode.",
)

# Define the main CLI application.
app = typer.Typer()


@app.command()
def train(
    run: str = typer.Option(None, "--run", "-r", help=get_help("run")),
    env: str = typer.Option(None, "--env", "-e", help=train_help.get("env")),
    config: str = typer.Option("{}", "--config", "-c", help=get_help("config")),
    config_file: str = typer.Option(
        None, "--config-file", "-f", help=train_help.get("config_file")
    ),
    stop: str = typer.Option("{}", "--stop", "-s", help=get_help("stop")),
    experiment_name: str = typer.Option(
        "default", "--experiment-name", "-n", help=train_help.get("experiment_name")
    ),
    v: bool = typer.Option(False, "--log-info", "-v", help=train_help.get("v")),
    vv: bool = typer.Option(False, "--log-debug", "-vv", help=train_help.get("vv")),
    resume: bool = typer.Option(False, help=train_help.get("resume")),
    num_samples: int = typer.Option(1, help=get_help("num_samples")),
    checkpoint_freq: int = typer.Option(0, help=get_help("checkpoint_freq")),
    checkpoint_at_end: bool = typer.Option(False, help=get_help("checkpoint_at_end")),
    local_dir: str = typer.Option(
        DEFAULT_RESULTS_DIR, help=train_help.get("local_dir")
    ),
    local_mode: bool = typer.Option(False, help=train_help.get("local_mode")),
    restore: str = typer.Option(None, help=get_help("restore")),
    framework: Framework = typer.Option(None, help=train_help.get("framework")),
    resources_per_trial: str = typer.Option(None, help=get_help("resources_per_trial")),
    sync_on_checkpoint: bool = typer.Option(False, help=get_help("sync_on_checkpoint")),
    keep_checkpoints_num: int = typer.Option(
        None, help=get_help("keep_checkpoints_num")
    ),
    checkpoint_score_attr: str = typer.Option(
        "training_iteration", help=get_help("sync_on_checkpoint")
    ),
    export_formats: str = typer.Option(None, help=get_help("export_formats")),
    max_failures: int = typer.Option(3, help=get_help("max_failures")),
    scheduler: str = typer.Option("FIFO", help=get_help("scheduler")),
    scheduler_config: str = typer.Option("{}", help=get_help("scheduler_config")),
    ray_address: str = typer.Option(None, help=train_help.get("ray_address")),
    ray_ui: bool = typer.Option(False, help=train_help.get("ray_ui")),
    ray_num_cpus: int = typer.Option(None, help=train_help.get("ray_num_cpus")),
    ray_num_gpus: int = typer.Option(None, help=train_help.get("ray_num_gpus")),
    ray_num_nodes: int = typer.Option(None, help=train_help.get("ray_num_nodes")),
    ray_object_store_memory: int = typer.Option(
        None, help=train_help.get("ray_object_store_memory")
    ),
    upload_dir: str = typer.Option("", help=train_help.get("upload_dir")),
    trace: bool = typer.Option(False, help=train_help.get("trace")),
    torch: bool = typer.Option(False, help=train_help.get("torch")),
    eager: bool = typer.Option(False, help=train_help.get("eager")),
):
    """Train a reinforcement learning agent.

    Training example via RLlib CLI:\n
        rllib train --run DQN --env CartPole-v1\n\n

    Grid search example via RLlib CLI:\n
        rllib train -f tuned_examples/ppo/cartpole-ppo.yaml\n\n

    Grid search example via executable:\n
        ./train.py -f tuned_examples/ppo/cartpole-ppo.yaml\n\n

    Note that -f overrides all other trial-specific command-line options.
    """
    # Note: we only import this here so that "framework" checks or initialising
    #   Ray don't slow down the boot-up of th CLI.
    from ray.rllib import train

    framework = framework.value if framework else None

    train.run(
        run=run,
        env=env,
        config=config,
        config_file=config_file,
        stop=stop,
        experiment_name=experiment_name,
        v=v,
        vv=vv,
        resume=resume,
        num_samples=num_samples,
        checkpoint_freq=checkpoint_freq,
        checkpoint_at_end=checkpoint_at_end,
        local_dir=local_dir,
        local_mode=local_mode,
        restore=restore,
        framework=framework,
        resources_per_trial=resources_per_trial,
        sync_on_checkpoint=sync_on_checkpoint,
        keep_checkpoints_num=keep_checkpoints_num,
        checkpoint_score_attr=checkpoint_score_attr,
        export_formats=export_formats,
        max_failures=max_failures,
        scheduler=scheduler,
        scheduler_config=scheduler_config,
        ray_address=ray_address,
        ray_ui=ray_ui,
        ray_num_cpus=ray_num_cpus,
        ray_num_gpus=ray_num_gpus,
        ray_num_nodes=ray_num_nodes,
        ray_object_store_memory=ray_object_store_memory,
        upload_dir=upload_dir,
        trace=trace,
        torch=torch,
        eager=eager,
    )

    # framework = framework.value  # get validated enum value


# TODO: make it possible for "checkpoint" to be an ID or so, while
#  remaining backward compatible.
@app.command()
def evaluate(
    checkpoint: str = typer.Argument(None, help=eval_help.get("checkpoint")),
    run: str = typer.Option(..., "--run", "-r", help=eval_help.get("run")),
    env: str = typer.Option(..., "--env", "-e", help=eval_help.get("env")),
    local_mode: bool = typer.Option(False, help=eval_help.get("local_mode")),
    render: bool = typer.Option(False, help=eval_help.get("render")),
    video_dir: str = typer.Option(None, help=eval_help.get("video_dir")),
    steps: int = typer.Option(10000, help=eval_help.get("steps")),
    episodes: int = typer.Option(0, help=eval_help.get("episodes")),
    out: str = typer.Option(None, help=eval_help.get("out")),
    config: str = typer.Option("{}", help=eval_help.get("config")),
    save_info: bool = typer.Option(False, help=eval_help.get("save_info")),
    use_shelve: bool = typer.Option(False, help=eval_help.get("use_shelve")),
    track_progress: bool = typer.Option(False, help=eval_help.get("track_progress")),
):
    """Roll out a reinforcement learning agent given a checkpoint argument.
    You have to provide an environment ("--env") an an RLlib algorithm ("--run") to
    evaluate your checkpoint.

    Example usage:\n\n

        rllib evaluate /tmp/ray/checkpoint_dir/checkpoint-0 --run DQN --env CartPole-v1
        --steps 1000000 --out rollouts.pkl
    """
    # Note: we defer loading of RLlib to make CLI start-up time instant.
    from ray.rllib import evaluate

    evaluate.run(
        checkpoint=checkpoint,
        run=run,
        env=env,
        local_mode=local_mode,
        render=render,
        video_dir=video_dir,
        steps=steps,
        episodes=episodes,
        out=out,
        config=config,
        save_info=save_info,
        use_shelve=use_shelve,
        track_progress=track_progress,
    )


@app.command()
def rollout(
    checkpoint: str = typer.Argument(None, help=eval_help.get("checkpoint")),
    run: str = typer.Option(..., "--run", "-r", help=eval_help.get("run")),
    env: str = typer.Option(..., "--env", "-e", help=eval_help.get("env")),
    local_mode: bool = typer.Option(False, help=eval_help.get("local_mode")),
    render: bool = typer.Option(False, help=eval_help.get("render")),
    video_dir: str = typer.Option(None, help=eval_help.get("video_dir")),
    steps: int = typer.Option(10000, help=eval_help.get("steps")),
    episodes: int = typer.Option(0, help=eval_help.get("episodes")),
    out: str = typer.Option(None, help=eval_help.get("out")),
    config: str = typer.Option("{}", help=eval_help.get("config")),
    save_info: bool = typer.Option(False, help=eval_help.get("save_info")),
    use_shelve: bool = typer.Option(False, help=eval_help.get("use_shelve")),
    track_progress: bool = typer.Option(False, help=eval_help.get("track_progress")),
):
    """This command is deprecated.
    Please use the "evaluate" command with the same arguments."""

    from ray.rllib.utils.deprecation import deprecation_warning

    deprecation_warning(old="rllib rollout", new="rllib evaluate", error=False)

    return evaluate(
        checkpoint=checkpoint,
        run=run,
        env=env,
        local_mode=local_mode,
        render=render,
        video_dir=video_dir,
        steps=steps,
        episodes=episodes,
        out=out,
        config=config,
        save_info=save_info,
        use_shelve=use_shelve,
        track_progress=track_progress,
    )


@app.callback()
def main_helper():
    """
    Welcome to the\n\n

    ██████╗ ██╗     ██╗     ██╗██████╗\n
    ██╔══██╗██║     ██║     ██║██╔══██╗\n
    ██████╔╝██║     ██║     ██║██████╔╝\n
    ██╔══██╗██║     ██║     ██║██╔══██╗  command\n
    ██║  ██║███████╗███████╗██║██████╔╝  line\n
    ╚═╝  ╚═╝╚══════╝╚══════╝╚═╝╚═════╝   interface\n\n

    Example usage for training:\n
        rllib train --run DQN --env CartPole-v1\n\n

    Example usage for evaluate (aka: "rollout"):\n
        rllib evaluate /trial_dir/checkpoint_000001/checkpoint-1 --run DQN
    --env CartPole-v1
    """


def cli():

    app()


if __name__ == "__main__":
    cli()
